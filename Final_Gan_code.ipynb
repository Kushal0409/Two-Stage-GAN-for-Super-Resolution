{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc04826-a8a0-46f3-8f12-234b1b1704a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchvision.models import vgg16\n",
    "\n",
    "# RRDBNet Components\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, growth_channels=32):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, growth_channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels + growth_channels, growth_channels, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels + 2 * growth_channels, growth_channels, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels + 3 * growth_channels, growth_channels, 3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels + 4 * growth_channels, in_channels, 3, padding=1)\n",
    "        self.lrelu = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.lrelu(self.conv1(x))\n",
    "        out2 = self.lrelu(self.conv2(torch.cat([x, out1], 1)))\n",
    "        out3 = self.lrelu(self.conv3(torch.cat([x, out1, out2], 1)))\n",
    "        out4 = self.lrelu(self.conv4(torch.cat([x, out1, out2, out3], 1)))\n",
    "        out5 = self.conv5(torch.cat([x, out1, out2, out3, out4], 1))\n",
    "        return out5 * 0.2 + x  # Residual scaling\n",
    "\n",
    "class RRDB(nn.Module):\n",
    "    def __init__(self, in_channels, growth_channels=32):\n",
    "        super(RRDB, self).__init__()\n",
    "        self.dense1 = DenseBlock(in_channels, growth_channels)\n",
    "        self.dense2 = DenseBlock(in_channels, growth_channels)\n",
    "        self.dense3 = DenseBlock(in_channels, growth_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.dense1(x)\n",
    "        out = self.dense2(out)\n",
    "        out = self.dense3(out)\n",
    "        return out * 0.2 + x  # Residual scaling\n",
    "\n",
    "class RRDBNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_rrdb_blocks=10):\n",
    "        super(RRDBNet, self).__init__()\n",
    "        self.conv_first = nn.Conv2d(in_channels, 64, 3, padding=1)\n",
    "        self.rrdb_blocks = nn.Sequential(*[RRDB(64) for _ in range(num_rrdb_blocks)])\n",
    "        self.conv_second = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.conv_upscale1 = nn.Conv2d(64, 256, 3, padding=1)  # 64 -> 256\n",
    "        self.pixel_shuffle = nn.PixelShuffle(2)  # Reduces channels from 256 -> 64\n",
    "        self.conv_upscale2 = nn.Conv2d(64, out_channels, 3, padding=1)  # Adjusted to 64 channels\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_first(x)\n",
    "        out = self.rrdb_blocks(out)\n",
    "        out = self.conv_second(out)\n",
    "        out = self.pixel_shuffle(self.conv_upscale1(out))  # upscale and reduce channels\n",
    "        out = self.conv_upscale2(out)  # Now expects 64 channels instead of 256\n",
    "        return self.tanh(out)\n",
    "\n",
    "# Two-Stage Generator Model\n",
    "class TwoStageGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TwoStageGenerator, self).__init__()\n",
    "        # First stage generator (e.g., generates an intermediate resolution)\n",
    "        self.generator_stage_1 = RRDBNet(in_channels=3, out_channels=3, num_rrdb_blocks=10)\n",
    "        # Second stage generator (refines the output of stage 1)\n",
    "        self.generator_stage_2 = RRDBNet(in_channels=3, out_channels=3, num_rrdb_blocks=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Stage 1 output\n",
    "        intermediate_output = self.generator_stage_1(x)\n",
    "        # Stage 2 refinement\n",
    "        final_output = self.generator_stage_2(intermediate_output)\n",
    "        return intermediate_output, final_output\n",
    "\n",
    "# Discriminator remains unchanged\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 4, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.lrelu1 = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 128, 4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.lrelu2 = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(128, 256, 4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.lrelu3 = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dense1 = nn.Linear(256, 1024)\n",
    "        self.lrelu4 = nn.LeakyReLU(0.2)\n",
    "        self.dense2 = nn.Linear(1024, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.lrelu1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.lrelu2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.lrelu3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.dense1(out)\n",
    "        out = self.lrelu4(out)\n",
    "        out = self.dense2(out)\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "# Custom Dataset class for image data\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, directory, target_size=(64, 64), scale_factor=4):\n",
    "        self.image_paths = glob.glob(os.path.join(directory, \"*.png\")) + \\\n",
    "                          glob.glob(os.path.join(directory, \"*.jpg\"))\n",
    "        self.target_size = target_size\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        high_res = img.resize(self.target_size, Image.LANCZOS)\n",
    "        low_res = img.resize((self.target_size[0]//self.scale_factor, self.target_size[1]//self.scale_factor), Image.LANCZOS)\n",
    "        low_res = low_res.resize(self.target_size, Image.LANCZOS)\n",
    "\n",
    "        high_res = np.array(high_res).astype(np.float32) / 127.5 - 1\n",
    "        low_res = np.array(low_res).astype(np.float32) / 127.5 - 1\n",
    "\n",
    "        return torch.from_numpy(low_res).permute(2, 0, 1), torch.from_numpy(high_res).permute(2, 0, 1)\n",
    "\n",
    "# VGG feature extractor for perceptual loss\n",
    "class VGGFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGFeatureExtractor, self).__init__()\n",
    "        model = vgg16(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(model.features)[:16])  # Extract features up to certain layer\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "# Loss functions\n",
    "def generator_loss(fake_output, fake_images_stage2, real_images, vgg):\n",
    "    bce = nn.BCELoss()\n",
    "    adversarial_loss = bce(fake_output, torch.ones_like(fake_output))\n",
    "\n",
    "    # Resize images to 224x224 for VGG\n",
    "    vgg_input_transform = nn.Upsample(size=(224, 224), mode='bilinear', align_corners=False)\n",
    "\n",
    "    fake_images_stage2_resized = vgg_input_transform(fake_images_stage2)\n",
    "    real_images_resized = vgg_input_transform(real_images)\n",
    "\n",
    "    # Extract content features\n",
    "    fake_features = vgg(fake_images_stage2_resized)\n",
    "    real_features = vgg(real_images_resized)\n",
    "    content_loss = nn.MSELoss()(fake_features, real_features)\n",
    "\n",
    "    return adversarial_loss + 100 * content_loss\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    bce = nn.BCELoss()\n",
    "    real_loss = bce(real_output, torch.ones_like(real_output))\n",
    "    fake_loss = bce(fake_output, torch.zeros_like(fake_output))\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "# Training loop\n",
    "def train_model(train_dir, val_dir, output_dir, epochs=100, device='cpu'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    generator = TwoStageGenerator().to(device)\n",
    "    discriminator = Discriminator().to(device)\n",
    "    vgg = VGGFeatureExtractor().to(device)\n",
    "\n",
    "    g_optimizer = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "\n",
    "    train_dataset = ImageDataset(train_dir)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, pin_memory=True)\n",
    "\n",
    "    val_dataset = ImageDataset(val_dir)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, pin_memory=True)\n",
    "\n",
    "    scaler = GradScaler()  # Gradient scaler for mixed precision\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Training\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        train_g_loss = 0\n",
    "        train_d_loss = 0\n",
    "        for low_res, high_res in train_loader:\n",
    "            low_res, high_res = low_res.to(device), high_res.to(device)\n",
    "\n",
    "            # Train discriminator\n",
    "            discriminator.zero_grad()\n",
    "            real_output = discriminator(high_res)\n",
    "            _, fake_images_stage2 = generator(low_res)\n",
    "            fake_output = discriminator(fake_images_stage2.detach())\n",
    "\n",
    "            with autocast():\n",
    "                d_loss = discriminator_loss(real_output, fake_output)\n",
    "            scaler.scale(d_loss).backward()\n",
    "            scaler.step(d_optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # Train generator\n",
    "            generator.zero_grad()\n",
    "            _, fake_images_stage2 = generator(low_res)\n",
    "            fake_output = discriminator(fake_images_stage2)\n",
    "\n",
    "            with autocast():\n",
    "                g_loss = generator_loss(fake_output, fake_images_stage2, high_res, vgg)\n",
    "            scaler.scale(g_loss).backward()\n",
    "            scaler.step(g_optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_g_loss += g_loss.item()\n",
    "            train_d_loss += d_loss.item()\n",
    "\n",
    "        print(f\"Generator Loss: {train_g_loss / len(train_loader):.4f}\")\n",
    "        print(f\"Discriminator Loss: {train_d_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        generator.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (low_res, high_res) in enumerate(val_loader):\n",
    "                low_res, high_res = low_res.to(device), high_res.to(device)\n",
    "                _, generated_images = generator(low_res)\n",
    "                generated_images = ((generated_images + 1) * 127.5).clamp(0, 255).permute(0, 2, 3, 1).byte().cpu().numpy()\n",
    "                for j, img in enumerate(generated_images):\n",
    "                    img_path = os.path.join(output_dir, f'epoch_{epoch+1}_sample_{i}_{j}.png')\n",
    "                    Image.fromarray(img).save(img_path)\n",
    "\n",
    "    torch.save(generator.state_dict(), os.path.join(output_dir, 'generator_final.pth'))\n",
    "    torch.save(discriminator.state_dict(), os.path.join(output_dir, 'discriminator_final.pth'))\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    train_dir = \"D:\\\\DIV2K_train_HR\\\\DIV2K_train_HR\"\n",
    "    val_dir = \"D:\\\\DIV2K_valid_HR\\\\DIV2K_valid_HR\"\n",
    "    output_dir = \"D:\\\\Images\\\\Code_output\"\n",
    "\n",
    "    train_model(train_dir, val_dir, output_dir, epochs=100, device='cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02434f6d-5f1c-4651-9ca3-80d22e5af601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jatin\\AppData\\Local\\Temp\\ipykernel_15028\\15859281.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device), strict=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from C:\\Users\\jatin\\Downloads\\generator_2.pth. \n",
      "Starting testing...\n",
      "Processing 1: flickr_cat_000008\n",
      "Saved result to D:\\Images\\results\\flickr_cat_000008_rlt.png\n",
      "Processing 2: flickr_cat_000011\n",
      "Saved result to D:\\Images\\results\\flickr_cat_000011_rlt.png\n",
      "Processing 3: flickr_cat_000016\n",
      "Saved result to D:\\Images\\results\\flickr_cat_000016_rlt.png\n",
      "Processing 4: flickr_cat_000056\n",
      "Saved result to D:\\Images\\results\\flickr_cat_000056_rlt.png\n",
      "Processing 5: flickr_cat_000076\n",
      "Saved result to D:\\Images\\results\\flickr_cat_000076_rlt.png\n",
      "Processing 6: flickr_cat_000080\n",
      "Saved result to D:\\Images\\results\\flickr_cat_000080_rlt.png\n",
      "Processing 7: flickr_cat_000096\n",
      "Saved result to D:\\Images\\results\\flickr_cat_000096_rlt.png\n",
      "Processing 8: flickr_cat_000108\n",
      "Saved result to D:\\Images\\results\\flickr_cat_000108_rlt.png\n",
      "Processing 9: flickr_cat_000123\n",
      "Saved result to D:\\Images\\results\\flickr_cat_000123_rlt.png\n",
      "Processing 10: flickr_cat_000136\n",
      "Saved result to D:\\Images\\results\\flickr_cat_000136_rlt.png\n",
      "Processing 11: flickr_cat_000152\n",
      "Saved result to D:\\Images\\results\\flickr_cat_000152_rlt.png\n",
      "Processing 12: flickr_cat_000162\n",
      "Saved result to D:\\Images\\results\\flickr_cat_000162_rlt.png\n",
      "Processing 13: flickr_cat_000165\n",
      "Saved result to D:\\Images\\results\\flickr_cat_000165_rlt.png\n",
      "Processing 14: flickr_cat_000174\n",
      "Saved result to D:\\Images\\results\\flickr_cat_000174_rlt.png\n",
      "Processing 15: flickr_cat_000175\n",
      "Saved result to D:\\Images\\results\\flickr_cat_000175_rlt.png\n",
      "Processing 16: flickr_cat_000176\n",
      "Saved result to D:\\Images\\results\\flickr_cat_000176_rlt.png\n",
      "Average PSNR: 35.28 dB\n",
      "Average SSIM: 0.9718\n",
      "All images processed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "sys.path.append('D:\\\\Gan_code')\n",
    "import RRDBNet_arch as arch\n",
    "\n",
    "# Model and device configuration\n",
    "model_path = 'C:\\\\Users\\\\jatin\\\\Downloads\\\\generator_2.pth'  # Path to your model weights\n",
    "device = torch.device('cpu')  # Use 'cuda' if you want to run on GPU\n",
    "\n",
    "# Test image folder and output directory\n",
    "test_img_folder = 'D:\\\\afhq test'  # Path to input images\n",
    "output_dir = 'D:\\\\Images\\\\results'       # Path to save results\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load the model\n",
    "model = arch.RRDBNet(3, 3, 64, 23, gc=32)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device), strict=True)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "print(f'Model loaded from {model_path}. \\nStarting testing...')\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "psnr_scores = []\n",
    "ssim_scores = []\n",
    "\n",
    "# Process each image in the folder\n",
    "idx = 0\n",
    "for path in glob.glob(osp.join(test_img_folder, '*.*')):  # Match all image files\n",
    "    idx += 1\n",
    "    base = osp.splitext(osp.basename(path))[0]\n",
    "    print(f'Processing {idx}: {base}')\n",
    "\n",
    "    # Read image\n",
    "    img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        print(f\"Error reading {path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Store original dimensions\n",
    "    original_height, original_width = img.shape[:2]\n",
    "    \n",
    "    # Store original image for metric calculations\n",
    "    img_original = img.copy()\n",
    "\n",
    "    img = img / 255.0  # Normalize to [0, 1]\n",
    "    img = torch.from_numpy(np.transpose(img[:, :, [2, 1, 0]], (2, 0, 1))).float()\n",
    "    img_LR = img.unsqueeze(0).to(device)\n",
    "\n",
    "    # Super-resolution inference\n",
    "    with torch.no_grad():\n",
    "        output = model(img_LR).data.squeeze().float().cpu().clamp_(0, 1).numpy()\n",
    "\n",
    "    # Convert back to image format\n",
    "    output = np.transpose(output[[2, 1, 0], :, :], (1, 2, 0))  # RGB\n",
    "    output = (output * 255.0).round().astype(np.uint8)  # Scale to [0, 255]\n",
    "    \n",
    "    # Resize output to match original dimensions\n",
    "    if output.shape[:2] != (original_height, original_width):\n",
    "        output = cv2.resize(output, (original_width, original_height))\n",
    "\n",
    "    # Save the output image\n",
    "    save_path = osp.join(output_dir, f'{base}_rlt.png')\n",
    "    cv2.imwrite(save_path, output)\n",
    "    print(f'Saved result to {save_path}')\n",
    "\n",
    "    # Convert images to grayscale for PSNR and SSIM\n",
    "    img_original_gray = cv2.cvtColor(img_original, cv2.COLOR_BGR2GRAY)\n",
    "    output_gray = cv2.cvtColor(output, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate PSNR\n",
    "    psnr_val = psnr(img_original_gray, output_gray, data_range=255)\n",
    "    psnr_scores.append(psnr_val)\n",
    "\n",
    "    # Calculate SSIM\n",
    "    ssim_val = ssim(img_original_gray, output_gray, data_range=255)\n",
    "    ssim_scores.append(ssim_val)\n",
    "\n",
    "# Report average metrics\n",
    "if psnr_scores:\n",
    "    print(f\"Average PSNR: {np.mean(psnr_scores):.2f} dB\")\n",
    "\n",
    "if ssim_scores:\n",
    "    print(f\"Average SSIM: {np.mean(ssim_scores):.4f}\")\n",
    "\n",
    "print(\"All images processed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3c815a-a02e-432a-8fde-809cd012a35e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
